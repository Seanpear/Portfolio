---
title: "Daily Exercise 11-12"
author: "Sean Pearson"
date: "2025-04-07"
format: html
execute:
  echo: true
---

## Question 1:

#### Part 1: Load the airquality dataset in R

```{r}
data(airquality)
library(tidyverse)
library(recipes)
```

```{r}
# Explore the structure
str(airquality)
summary(airquality)
```
```{r}
# Remove missing values
air_clean <- na.omit(airquality)
```

The airquality dataset contains daily air quality measurements taken in New York City from May to September 1973. It includes variables like: Wind, Temp, and Ozone.

#### Part 2: Perform a Shapiro-Wilk normality test
```{r}
# Shapiro-Wilk normality tests
shapiro.test(air_clean$Ozone)
shapiro.test(air_clean$Temp)
shapiro.test(air_clean$Solar.R)
shapiro.test(air_clean$Wind)
```

#### Part 3: What is the purpose of the Shapiro-Wilk test? 

The Shapiro-Wilk test checks whether a variable is normally distributed

#### Part 4: What are the null and alternative hypotheses for this test? 

Null hypothesis: The data comes from a normal distribution

Alternative hypothesis: The data does not come from a normal distribution

#### Part 5: Interpret the p-values

Normally Distributed: Temp, Wind
Not Normally Distributed: Ozone, Solar

## Question 2:

#### Part 1: Create a new column with case_when tranlating the Months into four seasons

```{r}
# Create a new column Season using case_when()
airquality <- airquality %>%
  mutate(Season = case_when(
    Month %in% c(11, 12, 1) ~ "Winter",
    Month %in% c(2, 3, 4) ~ "Spring",
    Month %in% c(5, 6, 7) ~ "Summer",
    Month %in% c(8, 9, 10) ~ "Fall",
    TRUE ~ "Unknown"
  ))
```

#### Part 2: Use table to figure out how many observations we have from each season

```{r}
# Count observations in each season
table(airquality$Season)
```

We have 61 observations for Fall and 92 for Summer.

## Question 3:

#### Part 1: Normalize the predictor variables

```{r}
# Remove rows with missing Month/Day 
airquality_clean <- airquality %>% drop_na(Month, Day)
```

```{r}
# Create a recipe
air_recipe <- recipe(Ozone ~ Temp + Solar.R + Wind + Season, data = airquality_clean) %>%
  step_impute_mean(all_numeric_predictors()) %>%  # imputes missing numeric values
  step_normalize(all_numeric_predictors()) %>%    # normalizes Temp, Solar.R, Wind
  step_dummy(Season)                              # one-hot encodes the Season column
```

#### Part 2: What is the purpose of normalizing data?

This is important when using models that are sensitive to the scale of predictors (like KNN, neural nets, and many regularized models), so that one variable doesn’t dominate just because of its units.

#### Part 3: What function can be used to impute missing values with the mean?

This equation can be used: step_impute_mean(all_numeric_predictors())

#### Part 4: Prep and bake the data to generate a processed dataset

```{r}  
# Prep the recipe 
air_recipe_prep <- prep(air_recipe)
```

```{r}
# Bake the prepped recipe 
processed_data <- bake(air_recipe_prep, new_data = NULL)
```

#### Part 5: Why is it necessary to both prep() and bake() the recipe? 

Prep() calculates things like the mean and standard deviation for normalization. Bake() applies what was learned during prep() to the actual dataset. It transforms my data using the recipe so I can use it for modeling or analysis.

## Question 4:

## Part 1: Fit a linear model using Ozone as the response variable and all other variables as predictors 

```{r}
# Remove missing values for a clean model fit
air_clean <- na.omit(airquality)
```

```{r}
# Fit linear model
lm_model <- lm(Ozone ~ ., data = air_clean)
```

#### Part 2: Interpret the model summary output

```{r}
# View model summary
summary(lm_model)
```

The model shows that temperature, wind, solar radiation, and month have statistically significant effects on ozone levels, based on their low p-values (all less than 0.05). Temperature and solar radiation have positive coefficients, meaning they increase ozone, while wind and month have negative coefficients, meaning they decrease ozone. The R-squared value of 0.63 means the model explains about 63% of the variation in ozone levels.

## Question 5: 

#### Part 1: Use broom::augment to suppliment the normalized data.frame with the fitted values and residuals
```{r}
library(ggpubr)
library(broom)
```

```{r}
a <- augment(lm_model, data = air_clean)
```

#### Part 2: Extract the residuals and visualize their distribution as a histogram and qqplot
```{r}
hist_plot <- ggplot(a, aes(x = .resid)) +
  geom_histogram(fill = "skyblue", color = "black", bins = 30) +
  labs(title = "Histogram of Residuals", x = "Residuals")
```

```{r}  
qq_plot <- ggplot(a, aes(sample = .resid)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "Q-Q Plot of Residuals")
```

#### Part 3: Use ggarange to plot this as one image and interpret what you see in them
```{r}  
# Put both plots side by side
ggarrange(hist_plot, qq_plot, ncol = 2, nrow = 1)
```
The histogram shows that most of the residuals are close to 0, but a few are much higher, meaning the model sometimes underpredicts Ozone levels. The Q-Q plot shows that the residuals mostly follow a normal pattern, but there are some bigger errors on the high end. Overall, the model does a pretty good job, but a few large errors stand out.

#### Part 4: Create a scatter plot of actual vs. predicted values using ggpubr
```{r}
# Scatter plot of actual vs predicted Ozone
ggscatter(a, x = "Ozone", y = ".fitted",
          add = "reg.line", conf.int = TRUE,
          cor.coef = TRUE, cor.method = "spearman",
          ellipse = TRUE)
```

#### Part 5: How strong of a model do you think this is?

The scatter plot shows a strong match between the actual and predicted Ozone values. The correlation is 0.84, which means the model is doing a good job. Most of the points are close to the line, so the predictions are pretty accurate. There are a few points that don’t fit as well, but overall the model works well.
